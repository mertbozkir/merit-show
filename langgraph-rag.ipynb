{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = GenericLoader.from_filesystem(\"/Users/mertbozkir/Code/langgraph-rag/langgraph/\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='class ChannelWrite(RunnablePassthrough):\\n    channels: Sequence[tuple[str, Optional[Runnable]]]\\n    \"\"\"\\n    Mapping of write channels to Runnables that return the value to be written,\\n    or None to skip writing.\\n    \"\"\"\\n\\n    class Config:\\n        arbitrary_types_allowed = True\\n\\n    def __init__(\\n        self,\\n        *,\\n        channels: Sequence[tuple[str, Optional[Runnable]]],\\n    ):\\n        super().__init__(func=self._write, afunc=self._awrite, channels=channels)\\n        self.name = f\"ChannelWrite<{\\',\\'.join(chan for chan, _ in self.channels)}>\"\\n\\n    def __repr_args__(self) -> Any:\\n        return [(\"channels\", self.channels)]\\n\\n    @property\\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\\n        return [\\n            ConfigurableFieldSpec(\\n                id=CONFIG_KEY_SEND,\\n                name=CONFIG_KEY_SEND,\\n                description=None,\\n                default=None,\\n                annotation=None,\\n            ),\\n        ]\\n\\n    def _write(self, input: Any, config: RunnableConfig) -> None:\\n        values = [\\n            (chan, r.invoke(input, config) if r else input) for chan, r in self.channels\\n        ]\\n        values = [\\n            write\\n            for write, chan in zip(values, self.channels)\\n            if chan[1] is None or write[1] is not None\\n        ]\\n\\n        self.do_write(config, **dict(values))\\n\\n    async def _awrite(self, input: Any, config: RunnableConfig) -> None:\\n        values = [\\n            (chan, await r.ainvoke(input, config) if r else input)\\n            for chan, r in self.channels\\n        ]\\n        values = [\\n            write\\n            for write, chan in zip(values, self.channels)\\n            if chan[1] is None or write[1] is not None\\n        ]\\n\\n        self.do_write(config, **dict(values))\\n\\n    @staticmethod\\n    def do_write(config: RunnableConfig, **values: Any) -> None:\\n        write: TYPE_SEND = config[\"configurable\"][CONFIG_KEY_SEND]\\n        write([(chan, val) for chan, val in values.items()])', metadata={'source': '/Users/mertbozkir/Code/langgraph-rag/langgraph/langgraph/pregel/write.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[4] # Example of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def create_agent_executor(agent_runnable, tools, input_schema=None):\n",
      "    if isinstance(tools, ToolExecutor):\n",
      "        tool_executor = tools\n",
      "    else:\n",
      "        tool_executor = ToolExecutor(tools)\n",
      "\n",
      "    state = _get_agent_state(input_schema)\n",
      "\n",
      "    # Define logic that will be used to determine which conditional edge to go down\n",
      "\n",
      "    def should_continue(data):\n",
      "        # If the agent outcome is an AgentFinish, then we return `exit` string\n",
      "        # This will be used when setting up the graph to define the flow\n",
      "        if isinstance(data[\"agent_outcome\"], AgentFinish):\n",
      "            return \"end\"\n",
      "        # Otherwise, an AgentAction is returned\n",
      "        # Here we return `continue` string\n",
      "        # This will be used when setting up the graph to define the flow\n",
      "        else:\n",
      "            return \"continue\"\n",
      "\n",
      "    def run_agent(data):\n",
      "        agent_outcome = agent_runnable.invoke(data)\n",
      "        return {\"agent_outcome\": agent_outcome}\n",
      "\n",
      "    async def arun_agent(data):\n",
      "        agent_outcome = await agent_runnable.ainvoke(data)\n",
      "        return {\"agent_outcome\": agent_outcome}\n",
      "\n",
      "    # Define the function to execute tools\n",
      "    def execute_tools(data):\n",
      "        # Get the most recent agent_outcome - this is the key added in the `agent` above\n",
      "        agent_action = data[\"agent_outcome\"]\n",
      "        output = tool_executor.invoke(agent_action)\n",
      "        return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
      "\n",
      "    async def aexecute_tools(data):\n",
      "        # Get the most recent agent_outcome - this is the key added in the `agent` above\n",
      "        agent_action = data[\"agent_outcome\"]\n",
      "        output = await tool_executor.ainvoke(agent_action)\n",
      "        return {\"intermediate_steps\": [(agent_action, str(output))]}\n",
      "\n",
      "    # Define a new graph\n",
      "    workflow = StateGraph(state)\n",
      "\n",
      "    # Define the two nodes we will cycle between\n",
      "    workflow.add_node(\"agent\", RunnableLambda(run_agent, arun_agent))\n",
      "    workflow.add_node(\"action\", RunnableLambda(execute_tools, aexecute_tools))\n",
      "\n",
      "    # Set the entrypoint as `agent`\n",
      "    # This means that this node is the first one called\n",
      "    workflow.set_entry_point(\"agent\")\n",
      "\n",
      "    # We now add a conditional edge\n",
      "    workflow.add_conditional_edges(\n",
      "        # First, we define the start node. We use `agent`.\n",
      "        # This means these are the edges taken after the `agent` node is called.\n",
      "        \"agent\",\n",
      "        # Next, we pass in the function that will determine which node is called next.\n",
      "        should_continue,\n",
      "        # Finally we pass in a mapping.\n",
      "        # The keys are strings, and the values are other nodes.\n",
      "        # END is a special node marking that the graph should finish.\n",
      "        # What will happen is we will call `should_continue`, and then the output of that\n",
      "        # will be matched against the keys in this mapping.\n",
      "        # Based on which one it matches, that node will then be called.\n",
      "        {\n",
      "            # If `tools`, then we call the tool node.\n",
      "            \"continue\": \"action\",\n",
      "            # Otherwise we finish.\n",
      "            \"end\": END,\n",
      "        },\n",
      "    )\n",
      "\n",
      "    # We now add a normal edge from `tools` to `agent`.\n",
      "    # This means that after `tools` is called, `agent` node is called next.\n",
      "    workflow.add_edge(\"action\", \"agent\")\n",
      "\n",
      "    # Finally, we compile it!\n",
      "    # This compiles it into a LangChain Runnable,\n",
      "    # meaning you can use it as you would any other runnable\n",
      "    return workflow.compile()\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I create langgraph multi-agent workflow??\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mertbozkir/Code/langgraph-rag/langrag/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=db)\n",
    "retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "template = \"\"\"Just give me the clean instructions and the code!\"\"\"\n",
    "   \n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history', return_messages=True,\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=OpenAI(),\n",
    "        retriever=db.as_retriever(),\n",
    "        memory=memory,\n",
    "        condense_question_prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mertbozkir/Code/langgraph-rag/langrag/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" To create a langgraph multi-agent workflow, you can use the `create_agent_executor` function. This function takes in an `agent_runnable` and a list of `tools`. It also has an optional `input_schema` parameter that can be used to define the initial state of the agent. The function will then create a `StateGraph` and define the logic for determining the next node to be called based on the agent's outcome. It will also handle the execution of `tools` and compile the workflow into a LangChain Runnable.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I create langgraph multi-agent workflow??\"\n",
    "result = conversation_chain(question)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" To create a langgraph multi-agent workflow, you can use the `create_agent_executor` function. \n",
    "This function takes in an `agent_runnable` and a list of `tools`. \n",
    "It also has an optional `input_schema` parameter that can be used to define the initial state of the agent. \n",
    "The function will then create a `StateGraph` and define the logic for determining the next node to be called based on the agent's outcome. \n",
    "It will also handle the execution of `tools` and compile the workflow into a LangChain Runnable.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
